{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algebra of Failure Inducing Input Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are trying to generate grammars that produce inputs that contain or does not specific behaviors (or complex combinations thereof) so, one can say things of the sort: I want to generate inputs with parenthesis, but discard everything with doubled parenthesis.\n",
    "\n",
    "While boolean grammars are the formalism for such things (e.g A & B & !C), and one can trivially construct multi-level recognizers for it (parse A, parse B, not parse C), generation is much harder, and as far as I can see, no one has found a way to easily generate inputs from such expressions (other than the generate and filter approach, which is really inefficient). Further, the boolean grammars are already beyond context free grammars, and our current techniques such as annotation of probabilities, and feedback on grammar nodes will not work on them. What we do here, is to recognize that for fault pattern additions and removal, one can stick to a context-free subset, and one can guarantee the properties of the resulting grammar.\n",
    "\n",
    "Other things we can likely do: Generate grammars that produce a given prefix or a given suffix, and combinations of those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often necessary to include multiple patterns in an input to trigger a fault. For example, a certain fault may occur only if a particular element is seen in advance, or a fault induced early on may only be triggered by a later element. Similarly, one may want to specify that a particular code element is covered, as well as a particular input element be/not be present (e.g. on bugfixes). Finally, one may also want to avoid triggering bugs that are already known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How\n",
    "\n",
    "We first extract the abstract syntactical patterns that correspond to given behaviors, and then generate a refinement grammar from the original grammar that follows the algebraic specifications.\n",
    "\n",
    "The main question being asked is, how to combine and negate fault patterns.\n",
    "\n",
    "We use `~F` for a nonterminal that is guaranteed not to contain a given fault, and `+F` for a nonterminal that is guaranteed to contain at least one instance of the given fault."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is it done?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the concept of an _abstract pattern_ form `DDSet`.\n",
    "\n",
    "### Abstract Pattern\n",
    "\n",
    "An _abstract pattern_ is a parse tree of a fault inducing input such that all non-causal subtrees are marked abstract. From an _abstract pattern_ we derive its _charecteristic node_.\n",
    "\n",
    "#### Charecteristic node\n",
    "\n",
    "A _charecteristic node_ of an _abstract pattern_ is the smallest subtree (and the corresponding node) that completely captures the concrete _terminal_ symbols in the _abstract pattern_.\n",
    "\n",
    "#### Linear grammar of a parse tree\n",
    "\n",
    "A _linear grammar_ of a parse tree is a grammar such that the grammar can produce only the given parse tree. It is constructed by marking each node in the parse tree with a unique suffix, and extracting the grammar of the suffixed tree. The _linear grammar_ of the _charecteristic node_ will produce the _exact_ string that produced the fault. The start symbol of the grammar is the nonterminal of the characteristic node.\n",
    "\n",
    "#### Linear abstract grammar\n",
    "\n",
    "This is derived from the _linear grammar_ of the _characteristic node_ where the nonterminals in the grammar that correspond to abstract nodes are replaced by the non-suffixed general nonterminals in the original grammar. The start symbol of the linear abstract grammar remains the same as the linear grammar of the characteristic node. The important thing to note here is that if one wishes to reproduce the given fault, the parse tree __should contain__ the nonterminals of the linear abstract grammar.\n",
    "\n",
    "#### Negated linear abstract grammar\n",
    "\n",
    "The linear abstract grammar for a fault is constructed from the linear abstract grammar of that fault by replacing the definitions one token at a time with a nonterminal guaranteed not to match the particular expansion in the linear grammar. To this set of rules is added the definitions from the original grammar that did not match the linear grammar.\n",
    "\n",
    "\n",
    "```\n",
    "<X> := <A> <B>\n",
    "<A> := A\n",
    "     | a <C>\n",
    "<B> := B\n",
    "     | b\n",
    "<C> := C\n",
    "     | C\n",
    "```\n",
    "Say the linear grammar is:\n",
    "```\n",
    "<X> := <A_1> <B_2>\n",
    "<A_1> := a <C_3>\n",
    "<B_2> := b\n",
    "<C_3> := c\n",
    "```\n",
    "\n",
    "Then, the negated linear grammar is (we define it using the symbol `^F` to avoid confusing with `+F`) --- The difference is that `+F` guarantees the full fault, while `^F` represents a partial fault.\n",
    "```\n",
    "<^X> := <^A_1> <B_2>\n",
    "      | <A_1> <^B_2>\n",
    "<A_1> := a <C_3>\n",
    "       | A\n",
    "<^A_1> := a <^C_3>\n",
    "        | A\n",
    "<B_2> := b\n",
    "<^B_2> := B\n",
    "<C_3> := c\n",
    "<^C_3> := C\n",
    "```\n",
    "If a particular nonterminal cannot be negated, then the rules that use that negated nonterminal is removed, and the nonterminals that have no rules are removed recursively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grammar with the guarantee that at least one instance of the fault will always be present (G+).\n",
    "\n",
    "We mark nonterminals that are guaranteed to contain at least a given single fault as `+F` and those nodes that are guaranteed to not contain that fault as `~F` where `<F>` is the original nonterminal.\n",
    "\n",
    "Given a grammar that starts with \n",
    "\n",
    "```\n",
    "<start> := <A> <B> <C>\n",
    "         ...\n",
    "```\n",
    "    \n",
    "The grammar will contain at least a single fault if that single rule is replaced by this set of rules\n",
    "\n",
    "```\n",
    "<1start> := <+A> <B> <C>\n",
    "         | <A> <+B> <C>\n",
    "         | <A> <B> <+C>\n",
    "...\n",
    "```\n",
    "Similarly, if one assumes that the nonterminal `<A>` can contain faults, and it has the following rules (nonterminals indicated by `<..>`)\n",
    "```\n",
    "<A> := x <P> <P> x\n",
    "     | x <P> x\n",
    "```\n",
    "then, the following definition is guaranteed to produce at least one instance of the fault in any expansion\n",
    "```\n",
    "<1A> := x <+P> <P> x\n",
    "     | x <P> <+P> x\n",
    "     | x <+P> x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grammar with the guarantee that no instance of the fault will be present (G-)\n",
    "\n",
    "Negation is simply negation of all nonterminals\n",
    "```\n",
    "<~A> := x <~P> <~P>\n",
    "      | x <~P>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grammar with the guarantee that at most one instance of the fault will be present (G*)\n",
    "\n",
    "```\n",
    "<1start> := <A> <~B> <~C>\n",
    "         | <~A> <B> <~C>\n",
    "         | <~A> <~B> <C>\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grammar with the guarantee that exactly one instance of the fault will be present\n",
    "\n",
    "```\n",
    "G_1 = G+ & G*\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base cases\n",
    "\n",
    "There are two base cases for this recursion. The first is when the nonterminal is the charecteristic node of the fault being inserted. In this case, the definition `<+F>` is simply the start symbol of the linear abstract grammar (and the linear abstract grammar is merged into the grammar). Similarly with the negation terminal.\n",
    "\n",
    "The second base case is when the nonterminal cannot produce the fault (e.g `<digit>` for producing a parenthesis). Here, the solution is simple. The nonterminal `<+F>` is defined as empty, and the rules that use that nonterminal is removed, and the nonterminals with empty rules are removed recursively as before. The negation `<~F>` is simply `<F>` because it is guaranteed not to produce a fault."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (|) two refined grammars.\n",
    "\n",
    "We generate grammars with the faults on boths sides. Next, we merge the grammars. Next, we remove any rule from the definition that is more refined than another rule in the same definition. A rule is more refined than another rule if 1) they both were derived from the same original rule from the original grammar (checked by looking at the stems and terminals) and 2) for any given token, the token at the same position of the other rule has a superset expansion. E.g a digit with expansion `[2, 4]` is a refinement of a digit with expansion `[1, 2, 3, 4, 5]`. (The fixpoint is computed). (This means that the disjunction of the original grammar with any fault inducing grammar will always be the original grammar.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (&) two refined grammars.\n",
    "\n",
    "Conjunction of two refined grammars is simply the conjunction of both start symbols. The conjunction of two matching (same stem) nonterminals is a conjunction of their matching rules (match with the same terminals and stem of nonterminals). Conjunction of two rules is single rule with each nonterminal representing a conjunction of two corresponding nonterminals at the corresponding places from the two rules. When there are multiple matching rules, the rules produced are all pairs.\n",
    "\n",
    "Note that one can generate an `atleast one fault grammar` (by skipping negation in the exactly one fault grammar) and an `atmost one fault grammar` (by skipping fault insertion in the exactly one fault grammar), and generate the `exactly one fault grammar` by generating a conjunction of both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The limitations are as follows:\n",
    "1. Only applicable to deterministic grammars (at least deterministic parts of the grammar)\n",
    "2. The fault patterns are neither sound nor complete w.r.t the failure (this is a limitation of the ddset fault pattern). \n",
    "   i.e:\n",
    "   The same failure may result from different patterns. Hence, negating one pattern does not mean negating that failure fully (not complete)\n",
    "   The same pattern in a different context may not result in a failure (not sound)\n",
    "4. Adding new fault patterns results in almost exponential increase (worst case) in the grammar rules of corresponding non terminals.\n",
    "5. We assume that rules for a nonterminal are non-redundant (If the same rule is repeated with same nonterminals, we can statically remove the more refined rule. If the nonterminals used are different, then it falls afoul of the deterministic grammar requirement)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The faults can from several different inputs. The idea is that the characterizing node, and abstraction removes the influences of the specific parse tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fault Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define an input from which we extract our patterns. Note that we do not use the predicate; rather we assume that we already have a few such predicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_input =  '1 + ((2 * 3 / 4))'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A context-free grammar is represented as a Python dict, with each nonterminal symbol forming a key, and each nonterminal _defined_ by a list of expansion rules. For example, the expression grammar for parsing arithmetic expressions is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPR_GRAMMAR = {'<start>': [['<expr>']],\n",
    " '<expr>': [['<term>', ' + ', '<expr>'],\n",
    "  ['<term>', ' - ', '<expr>'],\n",
    "  ['<term>']],\n",
    " '<term>': [['<factor>', ' * ', '<term>'],\n",
    "  ['<factor>', ' / ', '<term>'],\n",
    "  ['<factor>']],\n",
    " '<factor>': [['+', '<factor>'],\n",
    "  ['-', '<factor>'],\n",
    "  ['(', '<expr>', ')'],\n",
    "  ['<integer>', '.', '<integer>'],\n",
    "  ['<integer>']],\n",
    " '<integer>': [['<digit>', '<integer>'], ['<digit>']],\n",
    " '<digit>': [['0'], ['1'], ['2'], ['3'], ['4'], ['5'], ['6'], ['7'], ['8'], ['9']]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPR_START = '<start>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the convetion we used: Each nonterminal is enclosed in angle brackets. E.g. `<expr>`. We now define a function that can distinguish terminal symbols from nonterminals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `is_nt()` function checks if the given node is a terminal or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_nt(symbol):\n",
    "     return symbol and (symbol[0], symbol[-1]) == ('<', '>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the grammar, and an input, we can parse it into a derivation tree.\n",
    "The `Parser` below is from [fuzzingbook.org](https://www.fuzzingbook.org/html/Parser.html), and provides a generic context-free parser. This is present in the `src` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Parser import EarleyParser as Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we check that our parse succeeded? We can convert the derivation tree back to the original string and check for equality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tree_to_str()` function converts a derivation tree to its original string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_to_str(node):\n",
    "    name, children, *rest = node\n",
    "    if not children:\n",
    "        return name\n",
    "    return ''.join([tree_to_str(c) for c in children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_to_str(tree):\n",
    "    expanded = []\n",
    "    to_expand = [tree]\n",
    "    while to_expand:\n",
    "        (key, children, *rest), *to_expand = to_expand\n",
    "        if is_nt(key):\n",
    "            #assert children # not necessary\n",
    "            to_expand = list(children) + list(to_expand)\n",
    "        else:\n",
    "            assert not children\n",
    "            expanded.append(key)\n",
    "    return ''.join(expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ts = tree_to_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "expr_parser = Parser(EXPR_GRAMMAR, start_symbol=EXPR_START, canonical=True)\n",
    "expr_tree = list(expr_parser.parse(expr_input))[0]\n",
    "assert tree_to_str(expr_tree) == '1 + ((2 * 3 / 4))'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display\n",
    "\n",
    "While converting to strings are easy, it is unsatisfying. We want to make our output look pretty, and inspect the tree structure of the parsed tree. So we define graphical tree display (code from fuzzingbook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom(v, zoom=True):\n",
    "    # return v directly if you do not want to zoom out.\n",
    "    if zoom:\n",
    "        return Image(v.render(format='png'))\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisplayTree():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def extract_node(self, node, id):\n",
    "        symbol, children, *annotation = node\n",
    "        return symbol, children, ''.join(str(a) for a in annotation)\n",
    "    \n",
    "    def node_attr(self, dot, nid, symbol, ann):\n",
    "        dot.node(repr(nid), symbol + ' ')\n",
    "        \n",
    "    def edge_attr(self, dot, start_node, stop_node):\n",
    "        dot.edge(repr(start_node), repr(stop_node))\n",
    "        \n",
    "    def graph_attr(self, dot):\n",
    "        dot.attr('node', shape='plain')\n",
    "        \n",
    "    def display(self, derivation_tree):\n",
    "        counter = 0\n",
    "        def traverse_tree(dot, tree, id=0):\n",
    "            (symbol, children, annotation) = self.extract_node(tree, id)\n",
    "            self.node_attr(dot, id, symbol, annotation)\n",
    "            if children:\n",
    "                for child in children:\n",
    "                    nonlocal counter\n",
    "                    counter += 1\n",
    "                    child_id = counter\n",
    "                    self.edge_attr(dot, id, child_id)\n",
    "                    traverse_tree(dot, child, child_id)\n",
    "        dot = Digraph(comment=\"Derivation Tree\")\n",
    "        self.graph_attr(dot)\n",
    "        traverse_tree(dot, derivation_tree)\n",
    "        return dot\n",
    "    \n",
    "    def __call__(self, dt):\n",
    "        return self.display(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_tree = DisplayTree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to display the tree structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "zoom(display_tree(expr_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Fuzzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to define abstraction, we need to be able to generate values based on a grammar. Our fuzzer is able to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fuzzer:\n",
    "    def __init__(self, grammar):\n",
    "        self.grammar = grammar\n",
    "\n",
    "    def fuzz(self, key='<start>', max_num=None, max_depth=None):\n",
    "        raise NotImplemented()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fuzzer tries to randomly choose an expansion when more than one expansion is available. If however, it goes beyond max_depth, then it chooses the cheapest nodes. The cheapest nodes are those nodes with minimum further expansion (no recursion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LimitFuzzer(Fuzzer):\n",
    "    def symbol_cost(self, grammar, symbol, seen):\n",
    "        if symbol in self.key_cost: return self.key_cost[symbol]\n",
    "        if symbol in seen:\n",
    "            self.key_cost[symbol] = float('inf')\n",
    "            return float('inf')\n",
    "        v = min((self.expansion_cost(grammar, rule, seen | {symbol})\n",
    "                    for rule in grammar.get(symbol, [])), default=0)\n",
    "        self.key_cost[symbol] = v\n",
    "        return v\n",
    "\n",
    "    def expansion_cost(self, grammar, tokens, seen):\n",
    "        return max((self.symbol_cost(grammar, token, seen)\n",
    "                    for token in tokens if token in grammar), default=0) + 1\n",
    "\n",
    "    def nonterminals(self, rule):\n",
    "        return [t for t in rule if self.is_nt(t)]\n",
    "\n",
    "    def iter_gen_key(self, key, max_depth):\n",
    "        def get_def(t):\n",
    "            if self.is_nt(t):\n",
    "                return [t, None]\n",
    "            else:\n",
    "                return [t, []]\n",
    "\n",
    "        cheap_grammar = {}\n",
    "        for k in self.cost:\n",
    "            # should we minimize it here? We simply avoid infinities\n",
    "            rules = self.grammar[k]\n",
    "            min_cost = min([self.cost[k][str(r)] for r in rules])\n",
    "            #grammar[k] = [r for r in grammar[k] if self.cost[k][str(r)] == float('inf')]\n",
    "            cheap_grammar[k] = [r for r in self.grammar[k] if self.cost[k][str(r)] == min_cost]\n",
    "\n",
    "        root = [key, None]\n",
    "        queue = [(0, root)]\n",
    "        while queue:\n",
    "            # get one item to expand from the queue\n",
    "            (depth, item), *queue = queue\n",
    "            key = item[0]\n",
    "            if item[1] is not None: continue\n",
    "            grammar = self.grammar if depth < max_depth else cheap_grammar\n",
    "            chosen_rule = random.choice(grammar[key])\n",
    "            expansion = [get_def(t) for t in chosen_rule]\n",
    "            item[1] = expansion\n",
    "            for t in expansion: queue.append((depth+1, t))\n",
    "            #print(\"Fuzz: %s\" % key, len(queue), file=sys.stderr)\n",
    "        #print(file=sys.stderr)\n",
    "        return root\n",
    "\n",
    "    def gen_key(self, key, depth, max_depth):\n",
    "        if key not in self.grammar:\n",
    "            return (key, [])\n",
    "        if depth > max_depth:\n",
    "            #return self.gen_key_cheap_iter(key)\n",
    "            clst = sorted([(self.cost[key][str(rule)], rule) for rule in self.grammar[key]])\n",
    "            rules = [r for c,r in clst if c == clst[0][0]]\n",
    "        else:\n",
    "            rules = self.grammar[key]\n",
    "        return (key, self.gen_rule(random.choice(rules), depth+1, max_depth))\n",
    "\n",
    "    def gen_rule(self, rule, depth, max_depth):\n",
    "        return [self.gen_key(token, depth, max_depth) for token in rule]\n",
    "\n",
    "    def fuzz(self, key='<start>', max_depth=10):\n",
    "        self._s = self.iter_gen_key(key=key, max_depth=max_depth)\n",
    "        return self.tree_to_str(self._s)\n",
    "   \n",
    "    def is_nt(self, name):\n",
    "        return (name[0], name[-1]) == ('<', '>')\n",
    " \n",
    "    def tree_to_str(self, tree):\n",
    "        name, children = tree\n",
    "        if not self.is_nt(name): return name\n",
    "        return ''.join([tree_to_str(c) for c in children])\n",
    "\n",
    "    def tree_to_str(self, tree):\n",
    "        expanded = []\n",
    "        to_expand = [tree]\n",
    "        while to_expand:\n",
    "            (key, children, *rest), *to_expand = to_expand\n",
    "            if is_nt(key):\n",
    "                #assert children # not necessary\n",
    "                to_expand = children + to_expand\n",
    "            else:\n",
    "                assert not children\n",
    "                expanded.append(key)\n",
    "        return ''.join(expanded)\n",
    "\n",
    "\n",
    "    def __init__(self, grammar):\n",
    "        super().__init__(grammar)\n",
    "        self.key_cost = {}\n",
    "        self.cost = self.compute_cost(grammar)\n",
    "\n",
    "    def compute_cost(self, grammar):\n",
    "        cost = {}\n",
    "        for k in grammar:\n",
    "            cost[k] = {}\n",
    "            for rule in grammar[k]:\n",
    "                cost[k][str(rule)] = self.expansion_cost(grammar, rule, set())\n",
    "            if len(grammar[k]):\n",
    "                assert len([v for v in cost[k] if v != float('inf')]) > 0\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzer_expr = LimitFuzzer(EXPR_GRAMMAR)\n",
    "fuzzer_expr.fuzz(EXPR_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_grammar(g):\n",
    "    return {k:[[t for t in r] for r in g[k]] for k in g}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_rules(rules):\n",
    "    return sorted([list(r) for r in rules])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nullability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON=''\n",
    "def get_rules(g): return [(k, e) for k, a in g.items() for e in a]\n",
    "def get_terminals(g):\n",
    "    return set(t for k, expr in get_rules(g) for t in expr if t not in g)\n",
    "def fixpoint(f):\n",
    "    def helper(*args):\n",
    "        while True:\n",
    "            sargs = repr(args)\n",
    "            args_ = f(*args)\n",
    "            if repr(args_) == sargs:\n",
    "                return args\n",
    "            args = args_\n",
    "    return helper\n",
    "\n",
    "\n",
    "@fixpoint\n",
    "def nullable_(grules, e):\n",
    "    for A, expression in grules:\n",
    "        if all((token in e)  for token in expression): e |= {A}\n",
    "    return (grules, e)\n",
    "\n",
    "def nullable_nt(grammar):\n",
    "    return nullable_(get_rules(grammar), set())[1]\n",
    "\n",
    "def is_nullable(grammar, start):\n",
    "    return start in nullable_nt(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nullable_nt({'<a>': [['a'], ['<b>']], '<b>': [[]], '<c>': [['c']]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all_terminals(grammar):\n",
    "    return {k:[[t for t in r if is_nt(t)] for r in grammar[k]] for k in grammar} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_expr_grammar_tmp1 = remove_all_terminals(EXPR_GRAMMAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nullable_nt(_expr_grammar_tmp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nullable_nt(EXPR_GRAMMAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_g, empty_s = {'<start>': [['<expr>']], '<expr>': [['<expr>']]}, '<start>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nullable_nt(empty_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_nullable(empty_g, empty_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_nullable(EXPR_GRAMMAR, EXPR_START)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cfg_empty(grammar, start):\n",
    "    # first remove all terminals\n",
    "    null_g = remove_all_terminals(grammar)\n",
    "    # then check if start is nullable.\n",
    "    return not is_nullable(null_g, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cfg_empty(EXPR_GRAMMAR, EXPR_START)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cfg_empty(empty_g, empty_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_grammar(grammar, verbose=0):\n",
    "    r = 0\n",
    "    k = 0\n",
    "    for key in grammar:\n",
    "        k += 1\n",
    "        if verbose > -1: print(key,'::=')\n",
    "        for rule in grammar[key]:\n",
    "            r += 1\n",
    "            if verbose > 1:\n",
    "                pre = r\n",
    "            else:\n",
    "                pre = ''\n",
    "            if verbose > -1:\n",
    "                print('%s|   ' % pre, ' '.join([t if is_nt(t) else repr(t) for t in rule]))\n",
    "        if verbose > 0:\n",
    "            print(k, r)\n",
    "    print(k, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gs = show_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gs(EXPR_GRAMMAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsplit(token):\n",
    "    assert token[0], token[-1] == ('<', '>')\n",
    "    front, *back = token[1:-1].split(None, 1)\n",
    "    return front, ' '.join(back)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The stem\n",
    "\n",
    "The stem of a token is the original nonterminal it corresponds to, before refinement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(token):\n",
    "    return tsplit(token)[0].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(key):\n",
    "    return '<%s>' % stem(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The refinement\n",
    "\n",
    "The refinement of a token is the part after the initial space in a token which corresponds to how the refined token came to be (based on adding/negating other faults.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refinement(token):\n",
    "    return tsplit(token)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_most_general(token):\n",
    "    return normalize(token) == token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Node\n",
    "Finding nodes given the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_node(node, path):\n",
    "    name, children, *rest = node\n",
    "    if not path:\n",
    "        return node\n",
    "    p, *path = path\n",
    "    for i,c in enumerate(children):\n",
    "        if i == p:\n",
    "            return find_node(c, path)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert find_node(expr_tree, [0,1]) == (' + ', [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert find_node(expr_tree, [0,2,0,0,2]) == (')', [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Path to Key\n",
    "Get the path to where key is defined. This is not for parse trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_key(tree, key):\n",
    "    if tree is None: return None\n",
    "    name, children = tree\n",
    "    if key == name:\n",
    "        return []\n",
    "    for i,c in enumerate(children):\n",
    "        p = path_to_key(c, key)\n",
    "        if p is not None:\n",
    "            return [i] + p\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node to normalized rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_to_normalized_rule(children):\n",
    "    return [normalize(c[0]) if is_nt(c[0]) else c[0] for c in children]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_to_normalized_rule(rule):\n",
    "    return [normalize(t) if is_nt(t) else t for t in rule]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_tree(node, path, newnode):\n",
    "    if not path:\n",
    "        return newnode\n",
    "    name, children = node\n",
    "    hd, *subpath = path\n",
    "    assert hd < len(children)\n",
    "    new_children = []\n",
    "    for i,c in enumerate(children):\n",
    "        if i == hd:\n",
    "            c_ = replace_tree(c, subpath, newnode)\n",
    "        else:\n",
    "            c_ = c\n",
    "        new_children.append(c_)\n",
    "    return (name, new_children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tree_to_str(find_node(expr_tree, [0,2,0,0])) == '((2 * 3 / 4))'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tree_to_str(replace_tree(expr_tree, [0, 2, 0, 0], ('1', []))) == '1 + 1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate Tree\n",
    "\n",
    "Validating a parse tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_tree(tree, grammar):\n",
    "    def keys(arr):\n",
    "        return [a[0] for a in arr]\n",
    "    name, children, *rest = tree\n",
    "    if not is_nt(name): return True\n",
    "    \n",
    "    seen = False\n",
    "    for rule in grammar[name]:\n",
    "        if keys(children) == rule:\n",
    "            seen = True\n",
    "    assert seen, name + ' needs ' + repr(grammar[name])\n",
    "    for c in children:\n",
    "        validate_tree(c, grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_tree(expr_tree, EXPR_GRAMMAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    validate_tree(replace_tree(expr_tree, [0, 2, 0, 0], ('1', [])), EXPR_GRAMMAR)\n",
    "except AssertionError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt = replace_tree(expr_tree, [0, 2, 0, 0], ('<factor>', [('<integer>', [('<digit>',[('1', [])])])]))\n",
    "validate_tree(rt, EXPR_GRAMMAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tree_to_str(rt) == '1 + 1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove empty keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove keys that do not have a definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_empty_keys(g):\n",
    "    return [k for k in g if not g[k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_key(k, g):\n",
    "    new_g = {}\n",
    "    for k_ in g:\n",
    "        if k_ == k:\n",
    "            continue\n",
    "        else:\n",
    "            new_rules = []\n",
    "            for rule in g[k_]:\n",
    "                new_rule = []\n",
    "                for t in rule:\n",
    "                    if t == k:\n",
    "                        # skip this rule\n",
    "                        new_rule = None\n",
    "                        break\n",
    "                    else:\n",
    "                        new_rule.append(t)\n",
    "                if new_rule is not None:\n",
    "                    new_rules.append(new_rule)\n",
    "            new_g[k_] = new_rules\n",
    "    return new_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_keys(g):\n",
    "    new_g = copy_grammar(g)\n",
    "    removed_keys = []\n",
    "    empty_keys = find_empty_keys(new_g)\n",
    "    while empty_keys:\n",
    "        for k in empty_keys:\n",
    "            removed_keys.append(k)\n",
    "            new_g = remove_key(k, new_g)\n",
    "        empty_keys = find_empty_keys(new_g)\n",
    "    return new_g, removed_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Remove unused keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removes unused nonterminal keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unused_keys(grammar, start_symbol):\n",
    "    def strip_key(grammar, key, order):\n",
    "        rules = sort_rules(grammar[key])\n",
    "        old_len = len(order)\n",
    "        for rule in rules:\n",
    "            for token in rule:\n",
    "                if is_nt(token):\n",
    "                    if token not in order:\n",
    "                        order.append(token)\n",
    "        new = order[old_len:]\n",
    "        for ckey in new:\n",
    "            strip_key(grammar, ckey, order)\n",
    "    if start_symbol not in grammar:\n",
    "        return {}, []\n",
    "\n",
    "    order = [start_symbol]\n",
    "    strip_key(grammar, start_symbol, order)\n",
    "    if len(order) != len(grammar.keys()):\n",
    "        stripped = [k for k in grammar if k not in order]\n",
    "        #if stripped:\n",
    "        #    print(\"Stripping: %s\" % str(stripped))\n",
    "        faulty = [k for k in order if k not in grammar]\n",
    "        assert not faulty\n",
    "    new_g = {k: [list(r) for r in sort_rules(grammar[k])] for k in order}\n",
    "    return new_g, [k for k in grammar if k not in new_g]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_grammar(grammar, start_symbol):\n",
    "    def strip_key(grammar, key, order):\n",
    "        rules = sort_rules(grammar[key])\n",
    "        old_len = len(order)\n",
    "        for rule in rules:\n",
    "            for token in rule:\n",
    "                if is_nt(token):\n",
    "                    if token not in order:\n",
    "                        order.append(token)\n",
    "        new = order[old_len:]\n",
    "        for ckey in new:\n",
    "            strip_key(grammar, ckey, order)\n",
    "    if start_symbol not in grammar:\n",
    "        return {}, []\n",
    "\n",
    "    order = [start_symbol]\n",
    "    strip_key(grammar, start_symbol, order)\n",
    "    valid = True\n",
    "    if len(order) != len(grammar.keys()):\n",
    "        unused = [k for k in grammar if k not in order]\n",
    "        faulty = [k for k in order if k not in grammar]\n",
    "        if faulty or stripped:\n",
    "            print('faulty:', faulty)\n",
    "            print('unused:', unused)\n",
    "            return False\n",
    "    for (k1,k2) in zip(sorted(order), sorted(grammar.keys())):\n",
    "        if k1 != k2:\n",
    "            valid = False\n",
    "            print('order:', k1, 'grammar:', k2)\n",
    "        else:\n",
    "            print(k1, k2)\n",
    "    return valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove shadowed refined rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### is_A_more_refined_than_B\n",
    "\n",
    "Compare the positions of `ruleA` and `ruleB` in `porder` of base `grammar`.\n",
    "In the porder, we assume the most general ones will come first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_A_more_refined_than_B(ruleA, ruleB, porder):\n",
    "    if len(ruleA) != len(ruleB): return False\n",
    "    for a_, b_ in zip(ruleA, ruleB):\n",
    "        if not is_nt(a_) or not is_nt(b_):\n",
    "            if a_ != b_: return False\n",
    "            continue\n",
    "        a = normalize(a_) \n",
    "        b = normalize(b_)\n",
    "        if a != b: return False\n",
    "        if a not in porder or b not in porder: return None\n",
    "        pkA = path_to_key(porder[a], a_)\n",
    "        pkB = path_to_key(porder[b], b_)\n",
    "        if pkA is None or pkB is None: return None # we dont know.\n",
    "        pA = ' '.join([str(s) for s in pkA])\n",
    "        pB = ' '.join([str(s) for s in pkB])\n",
    "        # more general should be included in the more specific\n",
    "        if pB not in pA: return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_porder = {'<a>' : ('<a>',[('<a 1>', [('<a 2>',[('<a 3>',[])])])]),\n",
    "             '<b>':  ('<b>',[('<b 1>', [('<b 2>',[('<b 3>',[])])])]),\n",
    "             '<c>':  ('<c>',[('<c 1>', [('<c 2>',[('<c 3>',[])])])])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not is_A_more_refined_than_B(['<a 1>', '<b 1>', '<c 1>'], ['<a 2>', '<b 2>', '<c 2>'], my_porder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert is_A_more_refined_than_B(['<a 3>', '<b 3>', '<c 3>'], ['<a 2>', '<b 2>', '<c 2>'], my_porder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert is_A_more_refined_than_B(['<a 2>', '<b 2>', '<c 3>'], ['<a 2>', '<b 2>', '<c 2>'], my_porder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not is_A_more_refined_than_B(['<a 3>', '<b 3>', '<c 1>'], ['<a 2>', '<b 2>', '<c 2>'], my_porder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get general rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_general_rule(ruleA, rules, porder):\n",
    "    unknown = 0\n",
    "    for r in rules:\n",
    "        v = is_A_more_refined_than_B(ruleA, r, porder)\n",
    "        if v is None:\n",
    "            # we dont know about this.\n",
    "            unknown += 1\n",
    "            continue\n",
    "        elif v:\n",
    "            return r, unknown\n",
    "    return None, unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_general_rule(['<a 1>', '<b 1>', '<c 1>'], [\n",
    "    ['<a 1>', '<b 1>', '<c 1>'],\n",
    "    ['<a 2>', '<b 2>', '<c 2>'],\n",
    "    ['<a 3>', '<b 3>', '<c 3>'],\n",
    "], my_porder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_general_rule(['<a 1>', '<b 1>', '<c 3>'], [\n",
    "    ['<a 1>', '<b 1>', '<c 1>'],\n",
    "    ['<a 2>', '<b 2>'],\n",
    "    ['<a 3>'],\n",
    "], my_porder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_general_rule(['<a 1>', '<b 2>'], [\n",
    "    ['<a 1>', '<b 1>', '<c 1>'],\n",
    "    ['<a 2>', '<b 2>'],\n",
    "    ['<a 3>'],\n",
    "], my_porder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Is keyA more refined than keyB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_keyA_more_refined_than_keyB(keyA, keyB, porder, grammar):\n",
    "    # essential idea of comparing two keys is this:\n",
    "    # One key is smaller than the other if for any given rule in the first, there exist another rule that is larger\n",
    "    # than that in the second key.\n",
    "    # a rule is smaller than another if all tokens in that rule is either equal (matching) or smaller than\n",
    "    # the corresponding token in the other.\n",
    "    \n",
    "    # if normalize(keyB) == keyB: return True # normalized key is always the top (and may not exist in grammar)\n",
    "    \n",
    "    A_rules = grammar[keyA]\n",
    "    B_rules = grammar[keyB]\n",
    "   \n",
    "    for A_rule in A_rules:\n",
    "        v,unk = get_general_rule(A_rule, B_rules, porder)\n",
    "        if v is None:\n",
    "            if unk:\n",
    "                return None # dont know\n",
    "            return False\n",
    "        # There is a more general rule than A_rule in B_rules\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Insert into partial order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_into_porder(my_key, porder, grammar):\n",
    "    def update_tree(my_key, tree, grammar):\n",
    "        if tree is None: return True, (my_key, [])\n",
    "        k, children = tree\n",
    "        if is_most_general(my_key):\n",
    "            if not is_most_general(k):\n",
    "                return True, (my_key, [tree])\n",
    "            else:\n",
    "                return False, tree\n",
    " \n",
    "        v = is_keyA_more_refined_than_keyB(my_key, k, porder, grammar)\n",
    "        if is_most_general(k): v = True\n",
    "        # if v is unknown...\n",
    "        if v: # we should go into the children\n",
    "            if not children:\n",
    "                #print('>', 0)\n",
    "                return True, (k, [(my_key, [])])\n",
    "            new_children = []\n",
    "            updated = False\n",
    "            for c in children:\n",
    "                u, c_ = update_tree(my_key, c, grammar)\n",
    "                if u: updated = True\n",
    "                new_children.append(c_)\n",
    "            #print('>', 1)\n",
    "            return updated, (k, new_children)\n",
    "        else:\n",
    "            #v = is_keyA_more_refined_than_keyB(k, my_key, porder, grammar)\n",
    "            if v:\n",
    "                #this should be the parent of tree\n",
    "                #print('>', 2)\n",
    "                return True, (my_key, [tree])\n",
    "            else:\n",
    "                # add as a sibling -- but only if we have evidence.\n",
    "                if v is not None:\n",
    "                    #print('>', 3)\n",
    "                    return True, (k, children + [(my_key, [])])\n",
    "                else:\n",
    "                    return False, tree\n",
    "    key = normalize(my_key)\n",
    "    updated, v = update_tree(my_key, porder.get(key, None), grammar)\n",
    "    if updated:\n",
    "        porder[key] = v\n",
    "    return updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Is key in partial order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_key_in_porder(key, tree):\n",
    "    if tree is None: return False\n",
    "    name, children = tree\n",
    "    if name == key:\n",
    "        return True\n",
    "    for c in children:\n",
    "        if is_key_in_porder(key, c):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Identify partial orders of nonterminals from a grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory, identifying partial orders is simple once you have the machinary for `and` and `neg`. To find if a given nonterminal `A` is more refined than `B`, do `A - B` and `B - A` and check which of these are empty. Since we do not have the machinery yet, doing it here without that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_partial_orders(grammar):\n",
    "    porder = {}\n",
    "    cont = True\n",
    "    while cont:\n",
    "        cont = False\n",
    "        for k in grammar:\n",
    "            nkey = normalize(k)\n",
    "            if is_key_in_porder(k, porder.get(nkey, None)):\n",
    "                continue\n",
    "            updated = insert_into_porder(k, porder, grammar)\n",
    "            if not updated:\n",
    "                continue\n",
    "            cont = True\n",
    "    #for k in grammar:\n",
    "    #    assert k in porder\n",
    "    return porder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "po = identify_partial_orders(EXPR_GRAMMAR); po"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_is_redundant(rule, rules, porder):\n",
    "    # a rule is redundant if there is another in the rules that is more general.\n",
    "    grule, unknown = get_general_rule(rule, [r for r in rules if r != rule], porder)\n",
    "    if grule:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_redundant_rules(grammar, porder=None):\n",
    "    if porder is None:\n",
    "        porder = identify_partial_orders(grammar)\n",
    "    else:\n",
    "        if porder == {}:\n",
    "            _porder = identify_partial_orders(grammar)\n",
    "            porder.update(_porder)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    new_g = {}\n",
    "    removed_rules = 0\n",
    "    for key in grammar:\n",
    "        ruleset = list({tuple(r) for r in grammar[key]})\n",
    "        cont = True\n",
    "        while cont:\n",
    "            cont = False\n",
    "            for rule in ruleset:\n",
    "                if rule_is_redundant(rule, ruleset, porder):\n",
    "                    ruleset = [r for r in ruleset if r != rule]\n",
    "                    removed_rules += 1\n",
    "                    cont = True\n",
    "                else:\n",
    "                    continue\n",
    "            new_g[key] = ruleset\n",
    "    return new_g, removed_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "g, n = remove_redundant_rules(EXPR_GRAMMAR);\n",
    "Gs(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grammar GC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grammar_gc(grammar, start_symbol, options=(1,2,3), log=False):\n",
    "    g = grammar\n",
    "    while True:\n",
    "        if 1 in options:\n",
    "            g0, unused_keys = remove_unused_keys(g, start_symbol)\n",
    "        else:\n",
    "            g0, unused_keys = grammar, []\n",
    "        for k in g0:\n",
    "            for rule in g0[k]:\n",
    "                for t in rule: assert type(t) is str\n",
    "        if 2 in options:\n",
    "            g1, empty_keys = remove_empty_keys(g0)\n",
    "        else:\n",
    "            g1, empty_keys = g0, []\n",
    "        for k in g1:\n",
    "            for rule in g1[k]:\n",
    "                for t in rule: assert type(t) is str\n",
    "        g = g1\n",
    "        if log:\n",
    "            print('GC: ', unused_keys, empty_keys)\n",
    "        if not (len(unused_keys) + len(empty_keys)):\n",
    "            break\n",
    " \n",
    "    if 3 in options:\n",
    "        g2, redundant_rules = remove_redundant_rules(g)\n",
    "    else:\n",
    "        g2, redundant_rules = g, 0\n",
    "    return g2, start_symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output that we get from `ddset` has nodes marked. So, we define a way to mark nodes as abstract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mark the abstract nodes\n",
    "\n",
    "Given a path, we mark the node as abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_path_abstract(tree, path):\n",
    "    name, children = find_node(tree, path)\n",
    "    new_tree = replace_tree(tree, path, (name, children, {'abstract': True}))\n",
    "    return new_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we locate a suitable node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_path_1 = [0,2,0,0,1,0,0,1]\n",
    "assert tree_to_str(find_node(expr_tree, abs_path_1)) == '2 * 3 / 4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "v = mark_path_abstract(expr_tree, abs_path_1); v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a tree with some nodes marked abstract, go through the tree, and mark everything else as concrete. Default is to mark a node as concrete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_concrete_r(tree):\n",
    "    name, children, *abstract_a = tree\n",
    "    abstract = {'abstract': False} if not abstract_a else abstract_a[0]\n",
    "    return (name, [mark_concrete_r(c) for c in children], abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = mark_concrete_r(v); t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A way to display the abstracted tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def till_abstract(node):\n",
    "    name, children, *rest = node\n",
    "    if rest[-1]['abstract']:\n",
    "        return (name + '*', [])\n",
    "    return (name, [till_abstract(c) for c in children], *rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom(display_tree(till_abstract(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Da(t):\n",
    "    return zoom(display_tree(till_abstract(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_t1_ = find_node(expr_tree, [0, 2])\n",
    "tree_to_str(abs_t1_), abs_t1_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_t1 = ('<start>', [abs_t1_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_tree(abs_t1, EXPR_GRAMMAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_abs_p1 = [0, 0, 0, 1, 0, 0, 1]\n",
    "Ts(find_node(abs_t1, t_abs_p1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a function to check if a given node is abstract or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_node_abstract(node):\n",
    "    name, children, *abstract_a = node\n",
    "    if not abstract_a:\n",
    "        return True\n",
    "    else:\n",
    "        return abstract_a[0]['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_to_str_a(tree):\n",
    "    name, children, *general_ = tree\n",
    "    if not is_nt(name): return name\n",
    "    if is_node_abstract(tree):\n",
    "        return name\n",
    "    return ''.join([tree_to_str_a(c) for c in children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ta = tree_to_str_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_to_str_a(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_tree1 = mark_concrete_r(mark_path_abstract(abs_t1, t_abs_p1)); abs_tree1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ta(abs_tree1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Da(abs_tree1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_abstract_nodes(tree, paths):\n",
    "    for path in paths:\n",
    "        tree = mark_path_abstract(tree, path)\n",
    "    return mark_concrete_r(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ta(mark_abstract_nodes(abs_t1, []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ta(mark_abstract_nodes(abs_t1, [t_abs_p1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding characterizing node\n",
    "\n",
    "A characterizing node is the lowest node that completely contains the given pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def find_charecterizing_node(tree):\n",
    "    name, children, gen = tree\n",
    "    if len(children) == 1:\n",
    "        return find_charecterizing_node(children[0])\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_tree_cnode1 = find_charecterizing_node(abs_tree1); abs_tree_cnode1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the `<factor>` node completely contains the fault pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_tree1[0], Ts(abs_tree1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_tree_cnode1[0], Ts(abs_tree_cnode1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding reachable keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_reachable_keys(grammar, key, reachable_keys=None, found_so_far=None):\n",
    "    if reachable_keys is None: reachable_keys = {}\n",
    "    if found_so_far is None: found_so_far = set()\n",
    "\n",
    "    for rule in grammar[key]:\n",
    "        for token in rule:\n",
    "            if not is_nt(token): continue\n",
    "            if token in found_so_far: continue\n",
    "            found_so_far.add(token)\n",
    "            if token in reachable_keys:\n",
    "                for k in reachable_keys[token]:\n",
    "                    found_so_far.add(k)\n",
    "            else:\n",
    "                keys = find_reachable_keys(grammar, token, reachable_keys, found_so_far)\n",
    "                # reachable_keys[token] = keys <- found_so_far contains results from earlier\n",
    "    return found_so_far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key in EXPR_GRAMMAR:\n",
    "    keys = find_reachable_keys(EXPR_GRAMMAR, key, {})\n",
    "    print(key, keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding recursive keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reachable_dict(grammar):\n",
    "    reachable = {}\n",
    "    for key in grammar:\n",
    "        keys = find_reachable_keys(grammar, key, reachable)\n",
    "        reachable[key] = keys\n",
    "    return reachable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reachable_dict(EXPR_GRAMMAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the grammar with failure keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to add our grammar the keys that are required to cause a failure. For that, we first extract the local grammar that reproduces the fault pattern "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_tree_cnode1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def mark_faulty_name(symbol, prefix, v):\n",
    "    return '<%s L%s_%s>'% (symbol[1:-1], prefix, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_faulty_nodes(node, prefix, counter=None):\n",
    "    if counter is None: counter = {}\n",
    "    symbol, children, *abstract = node\n",
    "    if is_node_abstract(node): # we dont markup further\n",
    "        return node\n",
    "    if symbol not in counter: counter[symbol] = 0\n",
    "    counter[symbol] += 1\n",
    "    v = str(counter[symbol])\n",
    "    if is_nt(symbol):\n",
    "        return (mark_faulty_name(symbol, prefix, v),\n",
    "                [mark_faulty_nodes(c, prefix, counter) for c in children],\n",
    "                *abstract)\n",
    "    else:\n",
    "        assert not children\n",
    "        return (symbol, children, *abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_tree(mark_faulty_nodes(abs_tree_cnode1, '1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_node1 = mark_faulty_nodes(abs_tree_cnode1, '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_node1 = c_node1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def faulty_node_to_grammar(tree, grammar=None):\n",
    "    if grammar is None: grammar = {}\n",
    "    if is_node_abstract(tree): return grammar\n",
    "    name, children, *rest = tree\n",
    "    tokens = []\n",
    "    if name not in grammar: grammar[name] = []\n",
    "    for c in children:\n",
    "        n, cs, *rest = c\n",
    "        tokens.append(n)\n",
    "        if is_nt(n):\n",
    "            faulty_node_to_grammar(c, grammar)\n",
    "    grammar[name].append(tuple(tokens))\n",
    "    return grammar, tree[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g, s = faulty_node_to_grammar(abs_tree_cnode1)\n",
    "Gs(g)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def faulty_node_to_linear_grammar(tree, prefix, grammar=None):\n",
    "    ltree = mark_faulty_nodes(tree, prefix)\n",
    "    return faulty_node_to_grammar(ltree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg1, ls1 = faulty_node_to_linear_grammar(abs_tree_cnode1, '1')\n",
    "Gs(lg1)\n",
    "ls1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_tree_abs1 = abs_tree1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_faulty1 = abs_tree_cnode1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negated linear grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a linear grammar and the correspoinding grammar, we produce a negated linear grammar for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negate_lkey(key):\n",
    "    name, pattern = tsplit(key)\n",
    "    return '<%s -%s>' % (name, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negate_key_at(rule, at):\n",
    "    new_rule = []\n",
    "    for i,key in enumerate(rule):\n",
    "        if i == at:\n",
    "            new_rule.append(negate_lkey(key))\n",
    "        else:\n",
    "            new_rule.append(key)\n",
    "    return new_rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A node is unrefined (opposite of refined) if it is not refined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_refined(key):\n",
    "    return (' ' in key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negated_linear_grammar(lkey, linear_grammar, base_grammar):\n",
    "    new_grammar = {}\n",
    "    for l_key in linear_grammar:\n",
    "        nl_key = negate_lkey(l_key)\n",
    "        new_grammar[nl_key] = []\n",
    "        l_rule = linear_grammar[l_key][0]\n",
    "        # find all rules that do not match, and add to new_grammar\n",
    "        for rule in base_grammar[normalize(l_key)]:\n",
    "            if rule_to_normalized_rule(rule) != rule_to_normalized_rule(l_rule):\n",
    "                new_grammar[nl_key].append(rule)\n",
    "        # now negate the rule one token at a time.\n",
    "        for i, token in enumerate(l_rule):\n",
    "            if not is_nt(token): continue\n",
    "            if not is_refined(token): continue\n",
    "            new_rule = negate_key_at(l_rule, i)\n",
    "            new_grammar[nl_key].append(new_rule)\n",
    "    return new_grammar, negate_lkey(lkey) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlg1, nls1 = negated_linear_grammar(ls1, lg1, EXPR_GRAMMAR)\n",
    "Gs(nlg1)\n",
    "nls1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding insertable positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a rule, and the faulty symbol, the positions in the rule where the fault can be inserted are all the non-terminals that will eventually reach the symbol of the faulty symbol. That is, if we have `<digit> + <expr>` as the expansion and the faulty symbol is `<factor*>` then, since `<digit>` can never reach `<factor>`, `0` is out, and so is `1` since it is a terminal symbol. Hence, only `<expr>` remains, which when expanded, one of the expansion paths will include a `<factor>`. Hence, here `[2]` is the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reachable_positions(rule, fkey, reachable):\n",
    "    positions = []\n",
    "    for i, token in enumerate(rule):\n",
    "        if not is_nt(token): continue\n",
    "        if fkey in reachable[token]:\n",
    "            positions.append(i)\n",
    "    return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reachable1 = reachable_dict(EXPR_GRAMMAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in EXPR_GRAMMAR:\n",
    "    print(k)\n",
    "    for rule in EXPR_GRAMMAR[k]:\n",
    "        v = get_reachable_positions(rule, '<factor>', reachable1)\n",
    "        print('\\t', rule, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert into key definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The essential idea is to make the rules in the grammar such that there is one fault position in each position.\n",
    "Take one rule at a time. For each token in the rule, get the reachable tokens. If the fsym is not in reachable tokens, then the falt cannot be inserted in that position. So get all positions for the rule that we can insert fsym in, and for each position, change the symbol for later insertion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FKey(str, Enum):\n",
    "    negate = 'NEGATE'\n",
    "    fault = 'FAULT'\n",
    "    atmost = 'ATMOST'\n",
    "    atleast = 'ATLEAST'\n",
    "    exactly = 'EXACTLY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_fkey_prefix(name, prefix, kind):\n",
    "    if kind == FKey.negate:\n",
    "        return \"<%s -F%s>\" % (name[1:-1], prefix)\n",
    "    elif kind == FKey.fault:\n",
    "        return \"<%s .F%s>\" % (name[1:-1], prefix)\n",
    "    elif kind == FKey.atmost:\n",
    "        return \"<%s *F%s>\" % (name[1:-1], prefix)\n",
    "    elif kind == FKey.atleast:\n",
    "        return \"<%s +F%s>\" % (name[1:-1], prefix)\n",
    "    elif kind == FKey.exactly:\n",
    "        return \"<%s F%s>\" % (name[1:-1], prefix)\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_into_key(grammar, key, fsym, prefix, reachable):\n",
    "    rules = grammar[key]\n",
    "    my_rules = []\n",
    "    for rule in grammar[key]:\n",
    "        positions = get_reachable_positions(rule, fsym, reachable)\n",
    "        if not positions: # make it len(positions) >= n if necessary\n",
    "            # skip this rule because we can not embed the fault here.\n",
    "            continue\n",
    "        else:\n",
    "            # at each position, insert the fsym\n",
    "            for pos in positions:\n",
    "                new_rule = [to_fkey_prefix(t, prefix, FKey.fault)\n",
    "                            if pos == p else t for p,t in enumerate(rule)]\n",
    "                my_rules.append(new_rule)\n",
    "    return (to_fkey_prefix(key, prefix, FKey.fault), my_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in EXPR_GRAMMAR:\n",
    "    fk, rules = insert_into_key(EXPR_GRAMMAR, key, '<factor>', '1', reachable1)\n",
    "    print(fk)\n",
    "    for r in rules:\n",
    "        print('    ', r)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_into_grammar(grammar, fsym, prefix_f, reachable):\n",
    "    new_grammar = {}\n",
    "    for key in grammar:\n",
    "        fk, rules = insert_into_key(grammar, key, fsym, prefix_f, reachable)\n",
    "        if not rules: continue # no applicable rules\n",
    "        if fk not in new_grammar:\n",
    "            new_grammar[fk] = []\n",
    "        new_grammar[fk].extend(rules)\n",
    "    return new_grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the final grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final steps are as follows:\n",
    "1. Add the fault node, and the child nodes to the grammar.\n",
    "2. Generate the faulty key definitions. This is done per key in the original grammar.\n",
    "3. Finally, connect the faulty key and fault node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atleast_one_fault_grammar(grammar, start_symbol, fault_node, f_idx):\n",
    "    def L_prefix(i): return str(i)\n",
    "    def F_prefix(i): return str(i)\n",
    "    prefix_l = L_prefix(f_idx)\n",
    "    prefix_f = F_prefix(f_idx)\n",
    "    key_f = fault_node[0]\n",
    "    # First, get the linear grammar\n",
    "\n",
    "    g0, fkey = faulty_node_to_linear_grammar(fault_node, prefix_l)\n",
    " \n",
    "    reachable = reachable_dict(grammar)\n",
    "    # the new grammar contains the faulty keys and their definitions.\n",
    "    # next, want to insert the fault prefix_f into each insertable positions. \n",
    "    # the insertable locations are those that can reach fsym\n",
    "    g1 = insert_into_grammar(grammar, key_f, prefix_f, reachable)\n",
    "\n",
    "    # now, the faulty key is an alternative to the original.\n",
    "    # We have to take care of one thing though. The `fkey` in the linear grammar should\n",
    "    # be replaced with fsym, but the definitions kept. This is because we want to preserve\n",
    "    # the rule patterns. We do not want normal expansions to go through since it may mean\n",
    "    # no fault inserted. However, we want self recursion to happen.\n",
    "    fsym = to_fkey_prefix(key_f, prefix_f, FKey.fault)\n",
    "\n",
    "    new_grammar = {**grammar, **g0, **g1}\n",
    "    new_rules = g0[fkey] # get the linear rule\n",
    "    \n",
    "    for rule in new_grammar[fsym]:\n",
    "        for token in rule:\n",
    "            if not is_nt(token): continue\n",
    "            if normalize(token) == key_f:\n",
    "                new_rules.append(rule) # self recursion. We want to keep this\n",
    "                break\n",
    " \n",
    "    new_grammar[fsym] = new_rules\n",
    "\n",
    "    return new_grammar, to_fkey_prefix(start_symbol, F_prefix(f_idx), FKey.fault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_faulty1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The guarantee is at least one fault per input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faulty1_grammar_, faulty1_start = atleast_one_fault_grammar(EXPR_GRAMMAR, EXPR_START, node_faulty1, 1)\n",
    "Gs(faulty1_grammar_, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "faulty1_grammar, faulty1_start = grammar_gc(faulty1_grammar_, faulty1_start)\n",
    "Gs(faulty1_grammar)\n",
    "faulty1_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faulty1_fuzzer = LimitFuzzer(faulty1_grammar)\n",
    "faulty1_parser = Parser(faulty1_grammar, canonical=True, start_symbol=faulty1_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    s = faulty1_fuzzer.fuzz(key=faulty1_start)\n",
    "    print(s)\n",
    "    for t in faulty1_parser.parse(s):\n",
    "        assert tree_to_str(t) == s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few parses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tree in faulty1_parser.parse('((2))'):\n",
    "    print(tree_to_str(tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tree in faulty1_parser.parse('((1 + 1))'):\n",
    "    print(tree_to_str(tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for tree in faulty1_parser.parse('1 + 2'):\n",
    "        print(tree)\n",
    "except SyntaxError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tree in faulty1_parser.parse('1 + ((3))'):\n",
    "    print(tree_to_str(tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing a fault"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question is, given a grammar, how do we make sure that a particular abstract pattern does not occur. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove from key\n",
    "\n",
    "We need to define the `remove_from_key` first. The idea is that the fault does not occur in any of the reachable nonterminals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_from_key(grammar, key, fsym, prefix, reachable):\n",
    "    rules = grammar[key]\n",
    "    my_rules = []\n",
    "    for rule in grammar[key]:\n",
    "        positions = get_reachable_positions(rule, fsym, reachable)\n",
    "        if not positions: # make it len(positions) >= n if necessary\n",
    "            # add this rule as is because we cannot embed the fault here.\n",
    "            my_rules.append(rule)\n",
    "        else:\n",
    "            # at each position, insert the fsym\n",
    "            new_rule = []\n",
    "            for pos, token in enumerate(rule):\n",
    "                t = to_fkey_prefix(rule[pos], prefix, FKey.negate) if pos in positions else token\n",
    "                new_rule.append(t)\n",
    "            my_rules.append(new_rule)\n",
    "    return (to_fkey_prefix(key, prefix, FKey.negate), my_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in EXPR_GRAMMAR:\n",
    "    fk, rules = remove_from_key(EXPR_GRAMMAR, key, '<factor>', '1', reachable1)\n",
    "    print(fk)\n",
    "    for r in rules:\n",
    "        print('    ', r)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove from grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_from_grammar(grammar, fsym, prefix_f, reachable):\n",
    "    new_grammar = {}\n",
    "    for key in grammar:\n",
    "        fk, rules = remove_from_key(grammar, key, fsym, prefix_f, reachable)\n",
    "        if not rules: continue # no applicable rules\n",
    "        if fk not in new_grammar:\n",
    "            new_grammar[fk] = []\n",
    "        new_grammar[fk].extend(rules)\n",
    "    return new_grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the final grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final steps are as follows:\n",
    "1. Add the fault node, and the child nodes to the grammar.\n",
    "2. Generate the faulty key definitions. This is done per key in the original grammar.\n",
    "3. Finally, connect the faulty key and fault node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_fault_grammar(grammar, start_symbol, fault_node, f_idx):\n",
    "    def L_prefix(i): return str(i)\n",
    "    def F_prefix(i): return str(i)\n",
    "    prefix_l = L_prefix(f_idx)\n",
    "    prefix_f = F_prefix(f_idx)\n",
    "    key_f = fault_node[0]\n",
    "    # First, get the linear grammar\n",
    "\n",
    "    g0, fkey0 = faulty_node_to_linear_grammar(fault_node, prefix_l)\n",
    "    g1, fkey1 = negated_linear_grammar(fkey0, g0, grammar)\n",
    " \n",
    "    reachable = reachable_dict(grammar)\n",
    "    # the new grammar contains the faulty keys and their definitions.\n",
    "    # next, want to insert the fault prefix_f into each insertable positions. \n",
    "    # the insertable locations are those that can reach fsym\n",
    "    g2 = remove_from_grammar(grammar, key_f, prefix_f, reachable)\n",
    "\n",
    "    # now, the faulty key is an alternative to the original.\n",
    "    # We have to take care of one thing though. The `fkey` in the linear grammar should\n",
    "    # be replaced with fsym, but the definitions kept. This is because we want to preserve\n",
    "    # the rule patterns. We do not want normal expansions to go through since it may mean\n",
    "    # no fault inserted. However, we want self recursion to happen.\n",
    "    fsym = to_fkey_prefix(key_f, prefix_f, FKey.negate)\n",
    "\n",
    "    new_grammar = {**grammar, **g1, **g2}\n",
    "    new_rules = g1[fkey1] # get the linear rule\n",
    "    \n",
    "    for rule in new_grammar[fsym]:\n",
    "        for token in rule:\n",
    "            if not is_nt(token): continue\n",
    "            if normalize(token) == key_f:\n",
    "                new_rules.append(rule) # self recursion. We want to keep this\n",
    "                break\n",
    " \n",
    "    new_grammar[fsym] = new_rules\n",
    "\n",
    "    return new_grammar, to_fkey_prefix(start_symbol, F_prefix(f_idx), FKey.negate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_faulty1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nfaulty1_grammar_, nfaulty1_start = no_fault_grammar(EXPR_GRAMMAR, EXPR_START, node_faulty1, 1)\n",
    "Gs(nfaulty1_grammar_, -1)\n",
    "nfaulty1_grammar, nfaulty1_start = grammar_gc(nfaulty1_grammar_, nfaulty1_start)\n",
    "Gs(nfaulty1_grammar)\n",
    "nfaulty1_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfaulty1_fuzzer = LimitFuzzer(nfaulty1_grammar)\n",
    "nfaulty1_parser = Parser(nfaulty1_grammar, canonical=True, start_symbol=nfaulty1_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    s = nfaulty1_fuzzer.fuzz(key=nfaulty1_start)\n",
    "    print(s)\n",
    "    for t in nfaulty1_parser.parse(s):\n",
    "        assert tree_to_str(t) == s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few parses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for tree in nfaulty1_parser.parse('((2))'):\n",
    "        print(tree_to_str(tree))\n",
    "except SyntaxError as e:\n",
    "     print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for tree in nfaulty1_parser.parse('((1 + 1))'):\n",
    "        print(tree_to_str(tree))\n",
    "except SyntaxError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tree in nfaulty1_parser.parse('1 + 2'):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for tree in nfaulty1_parser.parse('1 + ((3))'):\n",
    "        print(tree_to_str(tree))\n",
    "except SyntaxError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## At most one fault"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove except one from key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_except_one_from_key(grammar, key, fsym, prefix, reachable):\n",
    "    rules = grammar[key]\n",
    "    my_rules = []\n",
    "    for rule in grammar[key]:\n",
    "        positions = get_reachable_positions(rule, fsym, reachable)\n",
    "        if not positions: # make it len(positions) >= n if necessary\n",
    "            # add this rule as is because we can not embed the fault here.\n",
    "            my_rules.append(rule)\n",
    "        else:\n",
    "            # skip pos for each rule\n",
    "            for pos in positions:\n",
    "                new_rule = []\n",
    "                for p,token in enumerate(rule):\n",
    "                    if (p in positions):\n",
    "                        if (p != pos): # at p position, there _may be_ a fault, but not in other places\n",
    "                            new_rule.append(to_fkey_prefix(rule[p], prefix, FKey.negate))\n",
    "                        else:\n",
    "                            # change to FKey.exactly to make it exactly\n",
    "                            new_rule.append(to_fkey_prefix(rule[p], prefix, FKey.atmost))\n",
    "                    else:\n",
    "                        new_rule.append(token)\n",
    "                my_rules.append(new_rule)\n",
    "    return (to_fkey_prefix(key, prefix, FKey.atmost), my_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove except one from grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_except_one_from_grammar(grammar, fsym, prefix_f, reachable):\n",
    "    new_grammar = {}\n",
    "    for key in grammar:\n",
    "        fk, rules = remove_except_one_from_key(grammar, key, fsym, prefix_f, reachable)\n",
    "        if not rules: continue # no applicable rules\n",
    "        if fk not in new_grammar:\n",
    "            new_grammar[fk] = []\n",
    "        new_grammar[fk].extend(rules)\n",
    "    return new_grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atmost one fault grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atmost_one_fault_grammar(grammar, start_symbol, fault_node, f_idx):\n",
    "    def L_prefix(i): return str(i)\n",
    "    def F_prefix(i): return str(i)\n",
    "    prefix_l = L_prefix(f_idx)\n",
    "    prefix_f = F_prefix(f_idx)\n",
    "    key_f = fault_node[0]\n",
    "    # First, get the linear grammar\n",
    "\n",
    "    g1, fkey1  = faulty_node_to_linear_grammar(fault_node, prefix_l)\n",
    "    g2, fkey2  = negated_linear_grammar(fkey1, g1, grammar)\n",
    "\n",
    "    reachable = reachable_dict(grammar)\n",
    "    # the new grammar contains the faulty keys and their definitions.\n",
    "    # next, want to insert the fault prefix_f into each insertable positions. \n",
    "    # the insertable locations are those that can reach fsym\n",
    "    g3 = remove_except_one_from_grammar(grammar, key_f, prefix_f, reachable)\n",
    "\n",
    "    g4 = remove_from_grammar(grammar, key_f, prefix_f, reachable)\n",
    "    \n",
    "    # now, the faulty key is an alternative to the original.\n",
    "    # We have to take care of one thing though. The `fkey` in the linear grammar should\n",
    "    # be replaced with fsym, but the definitions kept. This is because we want to preserve\n",
    "    # the rule patterns. We do not want normal expansions to go through since it may mean\n",
    "    # no fault inserted. However, we want self recursion to happen.\n",
    "    fsym = to_fkey_prefix(key_f, prefix_f, FKey.negate)\n",
    "\n",
    "    new_grammar = {**grammar, **g1, **g2, **g3, **g4}\n",
    "    new_rules = g2[fkey2] # get the linear rule\n",
    "    \n",
    "    for rule in new_grammar[fsym]:\n",
    "        for token in rule:\n",
    "            if not is_nt(token): continue\n",
    "            if normalize(token) == key_f:\n",
    "                new_rules.append(rule) # self recursion. We want to keep this\n",
    "                break\n",
    "    new_grammar[fsym] = new_rules\n",
    "    \n",
    "    # todo; we also want to insert the negative \n",
    "\n",
    "    return new_grammar, to_fkey_prefix(start_symbol, F_prefix(f_idx), FKey.atmost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nfaultya1_grammar_, nfaultya1_start = atmost_one_fault_grammar(EXPR_GRAMMAR, EXPR_START, node_faulty1, 1)\n",
    "Gs(nfaultya1_grammar_, -1)\n",
    "nfaultya1_grammar, nfaultya1_start = grammar_gc(nfaultya1_grammar_, nfaultya1_start)\n",
    "Gs(nfaultya1_grammar)\n",
    "nfaultya1_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfaultya1_fuzzer = LimitFuzzer(nfaultya1_grammar)\n",
    "nfaultya1_parser = Parser(nfaultya1_grammar, canonical=True, start_symbol=nfaultya1_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    s = nfaultya1_fuzzer.fuzz(key=nfaultya1_start)\n",
    "    print(s)\n",
    "    for t in nfaultya1_parser.parse(s):\n",
    "        assert tree_to_str(t) == s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few parses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for tree in nfaultya1_parser.parse('((2))'):\n",
    "        print(tree_to_str(tree))\n",
    "except SyntaxError as e:\n",
    "     print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for tree in nfaultya1_parser.parse('((1 + 1))'):\n",
    "        print(tree_to_str(tree))\n",
    "except SyntaxError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tree in nfaultya1_parser.parse('1 + 2'):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for tree in nfaultya1_parser.parse('1 + ((3))'):\n",
    "        print(tree_to_str(tree))\n",
    "except SyntaxError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exactly one fault grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_exactly_one_at_key(grammar, key, fsym, prefix, reachable):\n",
    "    rules = grammar[key]\n",
    "    my_rules = []\n",
    "    for rule in grammar[key]:\n",
    "        positions = get_reachable_positions(rule, fsym, reachable)\n",
    "        if not positions: # make it len(positions) >= n if necessary\n",
    "            # add this rule as is because we can not embed the fault here.\n",
    "            # my_rules.append(rule)\n",
    "            continue\n",
    "        else:\n",
    "            # skip pos for each rule\n",
    "            for pos in positions:\n",
    "                new_rule = []\n",
    "                for p,token in enumerate(rule):\n",
    "                    if (p in positions):\n",
    "                        if (p != pos): # at p position, there _should be_ a fault, but not in other places\n",
    "                            new_rule.append(to_fkey_prefix(rule[p], prefix, FKey.negate))\n",
    "                        else:\n",
    "                            new_rule.append(to_fkey_prefix(rule[p], prefix, FKey.exactly))\n",
    "                    else:\n",
    "                        new_rule.append(token)\n",
    "                my_rules.append(new_rule)\n",
    "    return (to_fkey_prefix(key, prefix, FKey.exactly), my_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_exactly_one_at_grammar(grammar, fsym, prefix_f, reachable):\n",
    "    new_grammar = {}\n",
    "    for key in grammar:\n",
    "        fk, rules = keep_exactly_one_at_key(grammar, key, fsym, prefix_f, reachable)\n",
    "        if not rules: continue # no applicable rules\n",
    "        if fk not in new_grammar:\n",
    "            new_grammar[fk] = []\n",
    "        new_grammar[fk].extend(rules)\n",
    "    return new_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exactly_one_fault_grammar(grammar, start_symbol, fault_node, f_idx, log=False):\n",
    "    def L_prefix(i): return str(i)\n",
    "    def F_prefix(i): return str(i)\n",
    "    prefix_l = L_prefix(f_idx)\n",
    "    prefix_f = F_prefix(f_idx)\n",
    "    key_f = fault_node[0]\n",
    "    # First, get the linear grammar\n",
    "\n",
    "    g1, fkey1  = faulty_node_to_linear_grammar(fault_node, prefix_l)\n",
    "    g2, fkey2  = negated_linear_grammar(fkey1, g1, grammar)\n",
    "\n",
    "    reachable = reachable_dict(grammar)\n",
    "    # the new grammar contains the faulty keys and their definitions.\n",
    "    # next, want to insert the fault prefix_f into each insertable positions. \n",
    "    # the insertable locations are those that can reach fsym\n",
    "    g3 = keep_exactly_one_at_grammar(grammar, key_f, prefix_f, reachable)\n",
    "\n",
    "    g4 = remove_from_grammar(grammar, key_f, prefix_f, reachable)\n",
    "    \n",
    "    # now, the faulty key is an alternative to the original.\n",
    "    # We have to take care of one thing though. The `fkey` in the linear grammar should\n",
    "    # be replaced with fsym, but the definitions kept. This is because we want to preserve\n",
    "    # the rule patterns. We do not want normal expansions to go through since it may mean\n",
    "    # no fault inserted. However, we _may_ want self recursion to happen.\n",
    "    # This is however, a tradeoff. If self recursions happen, then we need to make sure that\n",
    "    # the self recursion does not match the fault immediately.\n",
    "    fsym = to_fkey_prefix(key_f, prefix_f, FKey.exactly)\n",
    "    if log: print('Characterizing Node:', fsym)\n",
    "\n",
    "    new_grammar = {**grammar, **g1, **g2, **g3, **g4}\n",
    "    if log: print('Replacing Key:', fkey1)\n",
    "    new_rules = g1[fkey1] # get the linear rule\n",
    "    \n",
    "    for rule in new_grammar[fsym]:\n",
    "        # skip if the rule matches initial expansion of linear grammar\n",
    "        # TODO: this should actually be allowed so long as we split this rule\n",
    "        # into multiple rules, and at each point negate the linear grammar.\n",
    "        # Note, this is the base grammar. So, rule is already normalized.\n",
    "        fnode_children = fault_node[1]\n",
    "        if rule == node_to_normalized_rule(fnode_children):\n",
    "            new_rule = []\n",
    "            for p,token in enumerate(rule):\n",
    "                # only negate token p\n",
    "                if (p != pos): # at p position, there _should be_ a fault, but not in other places\n",
    "                    new_rule.append(to_fkey_prefix(fnode_children[p][0], prefix_l, FKey.negate))\n",
    "                else:\n",
    "                    new_rule.append(token)\n",
    "                my_rules.append(new_rule)\n",
    "\n",
    "            continue\n",
    "        for token in rule:\n",
    "            if not is_nt(token): continue\n",
    "            if normalize(token) == key_f:\n",
    "                # self recursion. We want to keep this,\n",
    "                new_rules.append(rule)\n",
    "                break\n",
    "    new_grammar[fsym] = new_rules\n",
    "    \n",
    "    # todo; we also want to insert the negative \n",
    "\n",
    "    return new_grammar, to_fkey_prefix(start_symbol, F_prefix(f_idx), FKey.exactly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efaultya1_grammar_, efaultya1_start = exactly_one_fault_grammar(EXPR_GRAMMAR, EXPR_START, node_faulty1, 1)\n",
    "Gs(efaultya1_grammar_, -1)\n",
    "efaultya1_grammar, efaultya1_start = grammar_gc(efaultya1_grammar_, efaultya1_start)\n",
    "Gs(efaultya1_grammar)\n",
    "efaultya1_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efaultya1_fuzzer = LimitFuzzer(efaultya1_grammar)\n",
    "efaultya1_parser = Parser(efaultya1_grammar, canonical=True, start_symbol=efaultya1_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    s = efaultya1_fuzzer.fuzz(key=efaultya1_start)\n",
    "    print(s)\n",
    "    for t in efaultya1_parser.parse(s):\n",
    "        assert tree_to_str(t) == s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjunction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exactly one fault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conj(k1, k2, simplify=False):\n",
    "    if is_nt(k1):\n",
    "        if not simplify:\n",
    "            return '<%s and(%s,%s)>' % (stem(k1), refinement(k1), refinement(k2))\n",
    "        \n",
    "        if refinement(k1) == refinement(k2):\n",
    "            return k1\n",
    "        elif not refinement(k1):\n",
    "            return k2\n",
    "        elif not  refinement(k2):\n",
    "            return k1\n",
    "        else:\n",
    "            return '<%s and(%s,%s)>' % (stem(k1), refinement(k1), refinement(k2))\n",
    "    else:\n",
    "        assert k1 == k2, 'k1: %s k2: %s' % (k1, k2)\n",
    "        return k1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def and_grammars(g1, s1, g2, s2):\n",
    "    g1_keys = g1.keys()\n",
    "    g2_keys = g2.keys()\n",
    "    g = {}\n",
    "    # now get the matching keys for each pair.\n",
    "    for k1,k2 in I.product(g1_keys, g2_keys):\n",
    "        # define and(k1, k2)\n",
    "        if normalize(k1) != normalize(k2): continue\n",
    "        # find matching rules\n",
    "        new_key_suffix = conj(k1, k2) \n",
    "        new_rules = []\n",
    "        for rule1 in g1[k1]:\n",
    "            for rule2 in g2[k2]:\n",
    "                if rule_to_normalized_rule(rule1) != rule_to_normalized_rule(rule2): continue\n",
    "                new_rule = [conj(t1, t2) for t1,t2 in zip(rule1, rule2)]\n",
    "                new_rules.append(new_rule) \n",
    "        g[new_key_suffix] = new_rules\n",
    "    return g, conj(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "and1_grammar_, and1_start = and_grammars(faulty1_grammar, faulty1_start, nfaultya1_grammar, nfaultya1_start)\n",
    "Gs(and1_grammar_, -1)\n",
    "and1_grammar, and1_start = grammar_gc(and1_grammar_, and1_start)\n",
    "Gs(and1_grammar)\n",
    "and1_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "and1_fuzzer = LimitFuzzer(and1_grammar)\n",
    "and1_parser = Parser(and1_grammar, canonical=True, start_symbol=and1_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    s = and1_fuzzer.fuzz(key=and1_start)\n",
    "    print(s)\n",
    "    for t in and1_parser.parse(s):\n",
    "        assert tree_to_str(t) == s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammar Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing `A - B` involves iterating through keys in A and B, and computing `k_A - k_B` for each pair. Then collect all rules in `A[k_A]` that do not match `B[k_B]`. These will appear as they are in the new. Next, for each rule in `B[k_B]`, compute the negation. These form the `-B[k_B]` list. Pair up matching rule pair in `A[k_A]` and `-B[k_B]` and `&` each token, and you are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def and_neg(k1, k2, simplify=False):\n",
    "    if not is_nt(k1):\n",
    "        assert k1 == k2\n",
    "        return k1\n",
    "    else:\n",
    "        if not simplify:\n",
    "            if refinement(k2) and refinement(k2)[0] == '-':\n",
    "                return '<%s %s%s>' % (stem(k1), refinement(k1), refinement(k2))\n",
    "            else:\n",
    "                #dependencies.append((k1, k2))\n",
    "                return '<%s and(%s,%s)>' % (stem(k1), refinement(k1), refinement(k2))\n",
    "        \n",
    "        if refinement(k2) == '':\n",
    "            return k1\n",
    "        elif refinement(k1) == '':\n",
    "            return k2\n",
    "        elif refinement(k2)[0] == '-':\n",
    "            return '<%s %s%s>' % (stem(k1), refinement(k1), refinement(k2))\n",
    "        else:\n",
    "            #dependencies.append((k1, k2))\n",
    "            return '<%s and(%s,%s)>' % (stem(k1), refinement(k1), refinement(k2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(k1, k2, simplify=False):\n",
    "    assert is_nt(k1)\n",
    "    if not simplify:\n",
    "        return '<%s %s-%s>' % (stem(k1), refinement(k1), refinement(k2))\n",
    "    \n",
    "    if not refinement(k1):\n",
    "        return '<%s -%s>' % (stem(k1), refinement(k2))\n",
    "    else:\n",
    "        return '<%s %s-%s>' % (stem(k1), refinement(k1), refinement(k2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative(k1):\n",
    "    assert is_nt(k1)\n",
    "    return '<%s -%s>' % (stem(k1), refinement(k1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rule_difference(keyA, rulesA, keyB, rulesB, log=False):\n",
    "    # collect all rules in A that do not match rulesB\n",
    "    # What should be done for unmatching? Remember that we need to `and`\n",
    "    # in `A` to all rules in neg(B). So, if there are rules in A that do not\n",
    "    # match rules in neg(B), they should not be present in the resulting grammar.\n",
    "    # Infact, the only rules in output should be those matching neg(B) and obviously A\n",
    "    # unmatching_A = [r for r in rulesA]\n",
    "    # for ruleB in rulesB:\n",
    "    #    unmatching_A = [r for r in unmatching_A\n",
    "    #                    if rule_to_normalized_rule(r) != rule_to_normalized_rule(ruleB)]\n",
    "    # if log:\n",
    "    #    for r in unmatching_A: print(keyA, 'unmatching:', r)\n",
    "    # compute negation for each rule in B[k_B]\n",
    "    neg_rulesB = []\n",
    "    for ruleB in rulesB:\n",
    "        for i,t in enumerate(ruleB):\n",
    "            if not is_nt(t): continue\n",
    "            if normalize(t) == t: continue # cannot negate top abstraction.\n",
    "            neg_ruleB_i = [t if i != j else negative(t) for j, t in enumerate(ruleB)]\n",
    "            if log: print(keyA, 'negrule:', neg_ruleB_i)\n",
    "            neg_rulesB.append(neg_ruleB_i)\n",
    "    # now pair up every rule in A[k_A] with every matching rule in -B[k_B]\n",
    "    new_rulesB = [] #unmatching_A\n",
    "    for (ruleA, ruleB) in I.product(rulesA, neg_rulesB):\n",
    "        # make sure that each terminal token matches and each nonterminal becomes a-b\n",
    "        if rule_to_normalized_rule(ruleA) != rule_to_normalized_rule(ruleB): continue\n",
    "        new_rule = [and_neg(ta, tb) for ta, tb in zip(ruleA, ruleB)]\n",
    "        if log: print(keyA, 'newrule:', new_rule)\n",
    "        new_rulesB.append(new_rule)\n",
    "    return new_rulesB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_grammars(gA, sA, gB, sB, log=False):\n",
    "    keys_A = gA.keys()\n",
    "    keys_B = gB.keys()\n",
    "    new_g = {}\n",
    "    and_g, and_s = and_grammars(gA, sA, gB, sB)\n",
    "    for (a, b) in I.product(keys_A, keys_B):\n",
    "        new_g[a] = gA[a]\n",
    "        new_g[b] = gB[b]\n",
    "        if normalize(a) != normalize(b): continue\n",
    "        new_g[diff(a, b)] = compute_rule_difference(a, gA[a], b, gB[b], log)\n",
    "    # we also need to take care of the new `and`s that we added.\n",
    "    new_g.update(and_g)\n",
    "    return new_g, diff(sA, sB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# diff1_grammar_, diff1_start = difference_grammars(EXPR_GRAMMAR, EXPR_START, and1_grammar, and1_start)\n",
    "# Gs(diff1_grammar_, -1)\n",
    "# diff1_grammar, diff1_start = grammar_gc(diff1_grammar_, diff1_start)\n",
    "# Gs(diff1_grammar)\n",
    "# diff1_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# diff2_grammar_, diff2_start = difference_grammars(and1_grammar, and1_start, EXPR_GRAMMAR, EXPR_START)\n",
    "# diff2_grammar, diff2_start = grammar_gc(diff1_grammar_, diff2_start)\n",
    "# Gs(diff2_grammar)\n",
    "# assert is_cfg_empty(diff2_grammar, diff2_start)\n",
    "# diff2_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disjunction\n",
    "\n",
    "The idea here is to produce a merge of both grammars. Unlike in `and` where we combined each rule pair, we will simply add both rulesets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disj(k1, k2, simplify=False):\n",
    "    if is_nt(k1):\n",
    "        if not simplify:\n",
    "            return '<%s or(%s,%s)>' % (stem(k1), refinement(k1), refinement(k2))\n",
    "            \n",
    "        if not refinement(k1):\n",
    "            return normalize(k1)\n",
    "        elif not refinement(k2):\n",
    "            return normalize(k2)\n",
    "        return '<%s or(%s,%s)>' % (stem(k1), refinement(k1), refinement(k2))\n",
    "    else:\n",
    "        assert k1 == k2, 'k1: %s k2: %s' % (k1, k2)\n",
    "        return k1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def or_grammars(g1, s1, g2, s2):\n",
    "    g1_keys = g1.keys()\n",
    "    g2_keys = g2.keys()\n",
    "    g = {}\n",
    "    # now get the matching keys for each pair.\n",
    "    for k in list(g1_keys) + list(g2_keys): \n",
    "         g[k] = g1.get(k, []) + g2.get(k, [])\n",
    "    g[disj(s1, s2)] = g1[s1] + g2[s2]\n",
    "    return g, disj(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "or1_grammar_, or1_start = or_grammars(EXPR_GRAMMAR, EXPR_START, and1_grammar, and1_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Gs(or1_grammar_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "or1_grammar, or1_start = grammar_gc(or1_grammar_, or1_start)\n",
    "Gs(or1_grammar)\n",
    "or1_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to identify and remove duplicate keys. Note that our `partial orders` are still primitive. We now have the machinery to do it right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "identify_partial_orders(or1_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying partial orders is simple once you have the machinary for `and` and `neg`. To find if a given nonterminal `A` is more refined than `B`, do `A-B`. This should be empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_keyA_more_refined_than_keyB(keyA, keyB, porder, grammar):\n",
    "    # essential idea of comparing two keys is this:\n",
    "    # One key is smaller than the other if for any given rule in the first, there exist another rule that is larger\n",
    "    # than that in the second key.\n",
    "    # a rule is smaller than another if all tokens in that rule is either equal (matching) or smaller than\n",
    "    # the corresponding token in the other.\n",
    "    \n",
    "    # if normalize(keyB) == keyB: return True # normalized key is always the top (and may not exist in grammar)\n",
    "   \n",
    "    A_B_g, A_B_s = difference_grammars(grammar, keyA, grammar, keyB)\n",
    "    if is_cfg_empty(A_B_g, A_B_s): #A is smaller, so A-B should be empty.\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    #if unk: return None # dont know\n",
    "    # There is a more general rule than A_rule in B_rules\n",
    "    #return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_into_porder(my_key, porder, grammar):\n",
    "    def update_tree(my_key, tree, grammar):\n",
    "        if tree is None: return True, (my_key, [])\n",
    "        k, children = tree\n",
    "        if is_most_general(my_key):\n",
    "            if not is_most_general(k):\n",
    "                return True, (my_key, [tree])\n",
    "            else:\n",
    "                return False, tree\n",
    " \n",
    "        v = is_keyA_more_refined_than_keyB(my_key, k, porder, grammar)\n",
    "        if is_most_general(k): v = True\n",
    "        # if v is unknown...\n",
    "        if v: # we should go into the children\n",
    "            if not children:\n",
    "                #print('>', 0)\n",
    "                return True, (k, [(my_key, [])])\n",
    "            new_children = []\n",
    "            updated = False\n",
    "            for c in children:\n",
    "                u, c_ = update_tree(my_key, c, grammar)\n",
    "                if u: updated = True\n",
    "                new_children.append(c_)\n",
    "            #print('>', 1)\n",
    "            return updated, (k, new_children)\n",
    "        else:\n",
    "            v = is_keyA_more_refined_than_keyB(k, my_key, porder, grammar)\n",
    "            if v:\n",
    "                #this should be the parent of tree\n",
    "                #print('>', 2)\n",
    "                return True, (my_key, [tree])\n",
    "            else:\n",
    "                # add as a sibling -- but only if we have evidence.\n",
    "                if v is not None:\n",
    "                    #print('>', 3)\n",
    "                    return True, (k, children + [(my_key, [])])\n",
    "                else:\n",
    "                    return False, tree\n",
    "    key = normalize(my_key)\n",
    "    updated, v = update_tree(my_key, porder.get(key, None), grammar)\n",
    "    if updated:\n",
    "        porder[key] = v\n",
    "    return updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identify_partial_orders(or1_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grammar_gc(grammar, start_symbol, options=(1,2,3), log=False):\n",
    "    g = grammar\n",
    "    po = {}\n",
    "    while True:\n",
    "        if 1 in options:\n",
    "            g0, unused_keys = remove_unused_keys(g, start_symbol)\n",
    "        else:\n",
    "            g0, unused_keys = grammar, []\n",
    "        for k in g0:\n",
    "            for rule in g0[k]:\n",
    "                for t in rule: assert type(t) is str\n",
    "        if 2 in options:\n",
    "            g1, empty_keys = remove_empty_keys(g0)\n",
    "        else:\n",
    "            g1, empty_keys = g0, []\n",
    "        for k in g1:\n",
    "            for rule in g1[k]:\n",
    "                for t in rule: assert type(t) is str\n",
    "\n",
    "        if 3 in options:\n",
    "            g2, redundant_rules = remove_redundant_rules(g1, po)\n",
    "        else:\n",
    "            g2, redundant_rules = g1, 0\n",
    "        g = g2\n",
    "\n",
    "        if log:\n",
    "            print('GC: ', unused_keys, empty_keys)\n",
    "        if not (len(unused_keys) + len(empty_keys) + redundant_rules):\n",
    "            break\n",
    "    return g, start_symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "or1_grammar, or1_start = grammar_gc(or1_grammar_, or1_start)\n",
    "Gs(or1_grammar)\n",
    "or1_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "and1_grammar, and1_start = grammar_gc(and1_grammar, and1_start)\n",
    "Gs(and1_grammar)\n",
    "and1_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# diff1_grammar, diff1_start = grammar_gc(diff1_grammar_, diff1_start)\n",
    "# Gs(diff1_grammar)\n",
    "# diff1_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fault A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exprA_input = '((1))'\n",
    "exprA_tree = list(expr_parser.parse(exprA_input))[0]\n",
    "tree_to_str(exprA_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_tree(exprA_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ns(expr, paths):\n",
    "    for path in paths:\n",
    "        n = find_node(expr, path)\n",
    "        print(n[0], tree_to_str(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_path_A = [0,0,0,1,0,0,1]\n",
    "Ns(exprA_tree, [abs_path_A])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FtA = mark_abstract_nodes(exprA_tree, [abs_path_A])\n",
    "Ta(FtA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Da(FtA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FpA = find_charecterizing_node(FtA)\n",
    "faultA_grammar_, faultA_start = exactly_one_fault_grammar(EXPR_GRAMMAR, EXPR_START, FpA, 'A')\n",
    "Gs(faultA_grammar_, -1)\n",
    "faultA_grammar, faultA_start = grammar_gc(faultA_grammar_, faultA_start)\n",
    "Gs(faultA_grammar)\n",
    "faultA_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faultA_fuzzer = LimitFuzzer(faultA_grammar)\n",
    "faultA_parser = Parser(faultA_grammar, canonical=True, start_symbol=faultA_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    s = faultA_fuzzer.fuzz(key=faultA_start)\n",
    "    print(s)\n",
    "    assert faultA_parser.can_parse(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for t in faultA_parser.parse('1'):\n",
    "        print(t)\n",
    "except SyntaxError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fault B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exprB_input = '0 + 0'\n",
    "exprB_tree = list(expr_parser.parse(exprB_input))[0]\n",
    "tree_to_str(exprB_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_path_B_a = [0,0]\n",
    "abs_path_B_b = [0,2,0]\n",
    "Ns(exprB_tree, [abs_path_B_a, abs_path_B_b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FtB = mark_abstract_nodes(exprB_tree, [abs_path_B_a, abs_path_B_b])\n",
    "Ta(FtB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Da(FtB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FpB = find_charecterizing_node(FtB)\n",
    "faultB_grammar_, faultB_start  = exactly_one_fault_grammar(EXPR_GRAMMAR, EXPR_START, FpB, 'B', log=True)\n",
    "Gs(faultB_grammar_, -1)\n",
    "faultB_grammar, faultB_start = grammar_gc(faultB_grammar_, faultB_start)\n",
    "Gs(faultB_grammar)\n",
    "faultB_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faultB_fuzzer = LimitFuzzer(faultB_grammar)\n",
    "faultB_parser = Parser(faultB_grammar, canonical=True, start_symbol=faultB_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    s = faultB_fuzzer.fuzz(key=faultB_start)\n",
    "    print(s)\n",
    "    assert faultB_parser.can_parse(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for t in faultB_parser.parse('1 - 2'):\n",
    "        print(t)\n",
    "except SyntaxError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fault C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exprC_input = '1 / 0'\n",
    "exprC_tree = list(expr_parser.parse(exprC_input))[0]\n",
    "tree_to_str(exprC_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_path_C = [0,0, 0]\n",
    "Ns(exprC_tree, [abs_path_C])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FtC = mark_abstract_nodes(exprC_tree, [abs_path_C])\n",
    "Da(FtC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "FpC = find_charecterizing_node(FtC)\n",
    "faultC_grammar_, faultC_start = exactly_one_fault_grammar(EXPR_GRAMMAR, EXPR_START, FpC, 'C')\n",
    "Gs(faultC_grammar_, -1)\n",
    "faultC_grammar, faultC_start = grammar_gc(faultC_grammar_, faultC_start)\n",
    "Gs(faultC_grammar)\n",
    "faultC_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faultC_fuzzer = LimitFuzzer(faultC_grammar)\n",
    "faultC_parser = Parser(faultC_grammar, canonical=True, start_symbol=faultC_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    s = faultC_fuzzer.fuzz(key=faultC_start)\n",
    "    print(s)\n",
    "    assert faultC_parser.can_parse(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for t in faultC_parser.parse('1 - 2'):\n",
    "        print(t)\n",
    "except SyntaxError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A & B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "AandB_grammar_, AandB_start = and_grammars(faultA_grammar, faultA_start, faultB_grammar, faultB_start)\n",
    "Gs(AandB_grammar_, -1)\n",
    "AandB_grammar, AandB_start = grammar_gc(AandB_grammar_, AandB_start)\n",
    "Gs(AandB_grammar)\n",
    "AandB_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AandB_fuzzer = LimitFuzzer(AandB_grammar)\n",
    "AandB_parser = Parser(AandB_grammar, canonical=True, start_symbol=AandB_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    s = AandB_fuzzer.fuzz(key=AandB_start, max_depth=0)\n",
    "    print(s)\n",
    "    print('A&B')\n",
    "    assert AandB_parser.can_parse(s)\n",
    "    print('A')\n",
    "    faultA_parser.can_parse(s)\n",
    "    print('B')\n",
    "    faultB_parser.can_parse(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A & C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AandC_grammar_, AandC_start = and_grammars(faultA_grammar, faultA_start, faultC_grammar, faultC_start)\n",
    "Gs(AandC_grammar_, -1)\n",
    "AandC_grammar, AandC_start = grammar_gc(AandC_grammar_, AandC_start)\n",
    "Gs(AandC_grammar)\n",
    "AandC_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AandC_fuzzer = LimitFuzzer(AandC_grammar)\n",
    "AandC_parser = Parser(AandC_grammar, canonical=True, start_symbol=AandC_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    s = AandC_fuzzer.fuzz(key=AandC_start, max_depth=0)\n",
    "    print(s)\n",
    "    print('A&C')\n",
    "    assert AandC_parser.can_parse(s)\n",
    "    print('A')\n",
    "    assert faultA_parser.can_parse(s)\n",
    "    print('C')\n",
    "    assert faultC_parser.can_parse(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B & C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BandC_grammar_, BandC_start = and_grammars(faultB_grammar, faultB_start, faultC_grammar, faultC_start)\n",
    "Gs(BandC_grammar_, -1)\n",
    "BandC_grammar, BandC_start = grammar_gc(BandC_grammar_, BandC_start)\n",
    "Gs(BandC_grammar)\n",
    "BandC_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BandC_fuzzer = LimitFuzzer(BandC_grammar)\n",
    "BandC_parser = Parser(BandC_grammar, canonical=True, start_symbol=BandC_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    s = BandC_fuzzer.fuzz(key=BandC_start, max_depth=0)\n",
    "    print(s)\n",
    "    print('B&C')\n",
    "    assert BandC_parser.can_parse(s)\n",
    "    print('B')\n",
    "    assert faultB_parser.can_parse(s)\n",
    "    print('C')\n",
    "    assert faultC_parser.can_parse(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disjunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A | B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AorB_grammar_, AorB_start = or_grammars(faultA_grammar, faultA_start, faultB_grammar, faultB_start)\n",
    "Gs(AorB_grammar_, -1)\n",
    "AorB_grammar, AorB_start = grammar_gc(AorB_grammar_, AorB_start)\n",
    "Gs(AorB_grammar)\n",
    "AorB_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AorB_fuzzer = LimitFuzzer(AorB_grammar)\n",
    "AorB_parser = Parser(AorB_grammar, canonical=True, start_symbol=AorB_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    s = AorB_fuzzer.fuzz(key=AorB_start)\n",
    "    print(s)\n",
    "    print('A|B')\n",
    "    assert AorB_parser.can_parse(s)\n",
    "    print('A', faultA_parser.can_parse(s))\n",
    "    print('B', faultB_parser.can_parse(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A | C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AorC_grammar_, AorC_start = or_grammars(faultA_grammar, faultA_start, faultC_grammar, faultC_start)\n",
    "Gs(AorC_grammar_, -1)\n",
    "AorC_grammar, AorC_start = grammar_gc(AorC_grammar_, AorC_start)\n",
    "Gs(AorC_grammar)\n",
    "AorC_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AorC_fuzzer = LimitFuzzer(AorC_grammar)\n",
    "AorC_parser = Parser(AorC_grammar, canonical=True, start_symbol=AorC_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    s = AorC_fuzzer.fuzz(key=AorC_start)\n",
    "    print(s)\n",
    "    print('A|C')\n",
    "    assert AorC_parser.can_parse(s)\n",
    "    print('A', faultA_parser.can_parse(s))\n",
    "    print('C', faultC_parser.can_parse(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B | C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BorC_grammar_, BorC_start = or_grammars(faultB_grammar, faultB_start, faultC_grammar, faultC_start)\n",
    "Gs(BorC_grammar_, -1)\n",
    "BorC_grammar, BorC_start = grammar_gc(BorC_grammar_, BorC_start)\n",
    "Gs(BorC_grammar)\n",
    "BorC_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BorC_fuzzer = LimitFuzzer(BorC_grammar)\n",
    "BorC_parser = Parser(BorC_grammar, canonical=True, start_symbol=BorC_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    s = BorC_fuzzer.fuzz(key=BorC_start)\n",
    "    print(s)\n",
    "    print('B|C')\n",
    "    assert BorC_parser.can_parse(s)\n",
    "    print('B', faultB_parser.can_parse(s))\n",
    "    print('C', faultC_parser.can_parse(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A - B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "AminusB_grammar_, AminusB_start = difference_grammars(faultA_grammar, faultA_start, faultB_grammar, faultB_start)\n",
    "Gs(AminusB_grammar_, -1)\n",
    "AminusB_grammar, AminusB_start = grammar_gc(AminusB_grammar_, AminusB_start)\n",
    "Gs(AminusB_grammar)\n",
    "AminusB_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate_grammar(AminusB_grammar, AminusB_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AminusB_fuzzer = LimitFuzzer(AminusB_grammar)\n",
    "AminusB_parser = Parser(AminusB_grammar, canonical=True, start_symbol=AminusB_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    s = AminusB_fuzzer.fuzz(key=AminusB_start)\n",
    "    print(s)\n",
    "    print('A-B')\n",
    "    assert AminusB_parser.can_parse(s)\n",
    "    print('A')\n",
    "    assert faultA_parser.can_parse(s)\n",
    "    print('-B')\n",
    "    assert not faultB_parser.can_parse(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A - C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AminusC_grammar_, AminusC_start = difference_grammars(faultA_grammar, faultA_start, faultC_grammar, faultC_start)\n",
    "Gs(AminusC_grammar_, -1)\n",
    "AminusC_grammar, AminusC_start = grammar_gc(AminusC_grammar_, AminusC_start)\n",
    "Gs(AminusC_grammar)\n",
    "AminusC_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AminusC_fuzzer = LimitFuzzer(AminusC_grammar)\n",
    "AminusC_parser = Parser(AminusC_grammar, canonical=True, start_symbol=AminusC_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    s = AminusC_fuzzer.fuzz(key=AminusC_start)\n",
    "    print(s)\n",
    "    print('A-C')\n",
    "    assert AminusC_parser.can_parse(s)\n",
    "    print('A')\n",
    "    assert faultA_parser.can_parse(s)\n",
    "    print('-C')\n",
    "    assert not faultC_parser.can_parse(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B - C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BminusC_grammar_, BminusC_start = difference_grammars(faultB_grammar, faultB_start, faultC_grammar, faultC_start)\n",
    "Gs(BminusC_grammar_, -1)\n",
    "BminusC_grammar, BminusC_start = grammar_gc(BminusC_grammar_, BminusC_start)\n",
    "Gs(BminusC_grammar)\n",
    "BminusC_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BminusC_fuzzer = LimitFuzzer(BminusC_grammar)\n",
    "BminusC_parser = Parser(BminusC_grammar, canonical=True, start_symbol=BminusC_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    s = BminusC_fuzzer.fuzz(key=BminusC_start)\n",
    "    print(s)\n",
    "    print('B-C')\n",
    "    assert BminusC_parser.can_parse(s)\n",
    "    print('B')\n",
    "    assert faultB_parser.can_parse(s)\n",
    "    print('-C')\n",
    "    assert not faultC_parser.can_parse(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
